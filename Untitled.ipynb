{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd613a4f-5223-43dd-8ca0-9b06b9118e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import ale_py\n",
    "import time, os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c206c2e-92eb-464e-b0f8-5b8e235ccc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "hidden_dim = 64\n",
    "gamma = 0.97\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.993\n",
    "epsilon_min = 0.05\n",
    "\n",
    "# Reward Parameters\n",
    "line_clear_reward = 2.5\n",
    "step_penalty = -0.03\n",
    "game_over_penalty = -5.0\n",
    "\n",
    "# Training Episodes\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7474d2-eb53-4439-8f37-ba1ec4f7e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_action_space', '_cached_spec', '_disable_render_order_enforcing', '_has_reset', '_is_protocol', '_metadata', '_np_random', '_np_random_seed', '_observation_space', '_saved_kwargs', 'action_space', 'class_name', 'close', 'env', 'get_wrapper_attr', 'has_reset', 'has_wrapper_attr', 'metadata', 'np_random', 'np_random_seed', 'observation_space', 'render', 'render_mode', 'reset', 'set_wrapper_attr', 'spec', 'step', 'unwrapped', 'wrapper_spec']\n",
      "5\n",
      "Episode 1: Total Reward = -24.319999999999975, Avg Actor Loss = -0.0041, Avg Critic Loss = 0.0247\n",
      "Episode 2: Total Reward = -8.419999999999993, Avg Actor Loss = -0.0440, Avg Critic Loss = 0.1353\n",
      "Episode 3: Total Reward = -21.679999999999875, Avg Actor Loss = -0.0098, Avg Critic Loss = 0.0286\n",
      "Episode 4: Total Reward = -17.839999999999918, Avg Actor Loss = -0.0141, Avg Critic Loss = 0.0370\n",
      "Episode 5: Total Reward = -22.399999999999903, Avg Actor Loss = -0.0104, Avg Critic Loss = 0.0273\n",
      "Episode 6: Total Reward = -17.599999999999923, Avg Actor Loss = -0.0136, Avg Critic Loss = 0.0379\n",
      "Episode 7: Total Reward = -21.679999999999875, Avg Actor Loss = -0.0109, Avg Critic Loss = 0.0284\n",
      "Episode 8: Total Reward = -18.919999999999895, Avg Actor Loss = -0.0130, Avg Critic Loss = 0.0342\n",
      "Episode 9: Total Reward = -20.719999999999857, Avg Actor Loss = -0.0116, Avg Critic Loss = 0.0301\n",
      "Episode 10: Total Reward = -27.200000000000085, Avg Actor Loss = -0.0067, Avg Critic Loss = 0.0216\n",
      "Episode 11: Total Reward = -23.719999999999953, Avg Actor Loss = -0.0098, Avg Critic Loss = 0.0254\n",
      "Episode 12: Total Reward = -18.559999999999903, Avg Actor Loss = -0.0135, Avg Critic Loss = 0.0350\n",
      "Episode 13: Total Reward = -23.839999999999957, Avg Actor Loss = -0.0098, Avg Critic Loss = 0.0256\n",
      "Episode 14: Total Reward = -18.079999999999913, Avg Actor Loss = -0.0141, Avg Critic Loss = 0.0364\n",
      "Episode 15: Total Reward = -23.479999999999944, Avg Actor Loss = -0.0099, Avg Critic Loss = 0.0257\n",
      "Episode 16: Total Reward = -8.029999999999994, Avg Actor Loss = -0.0583, Avg Critic Loss = 0.1451\n",
      "Episode 17: Total Reward = -20.11999999999987, Avg Actor Loss = -0.0122, Avg Critic Loss = 0.0313\n",
      "Episode 18: Total Reward = -23.479999999999944, Avg Actor Loss = -0.0100, Avg Critic Loss = 0.0259\n",
      "Episode 19: Total Reward = -7.639999999999997, Avg Actor Loss = -0.0671, Avg Critic Loss = 0.1660\n",
      "Episode 20: Total Reward = -22.519999999999907, Avg Actor Loss = -0.0105, Avg Critic Loss = 0.0271\n",
      "Episode 21: Total Reward = -19.63999999999988, Avg Actor Loss = -0.0126, Avg Critic Loss = 0.0325\n",
      "Episode 22: Total Reward = -7.849999999999996, Avg Actor Loss = -0.0614, Avg Critic Loss = 0.1481\n",
      "Episode 23: Total Reward = -20.95999999999985, Avg Actor Loss = -0.0115, Avg Critic Loss = 0.0295\n",
      "Episode 24: Total Reward = -19.519999999999882, Avg Actor Loss = -0.0127, Avg Critic Loss = 0.0324\n",
      "Episode 25: Total Reward = -7.549999999999998, Avg Actor Loss = -0.0693, Avg Critic Loss = 0.1696\n",
      "Episode 26: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0135, Avg Critic Loss = 0.0347\n",
      "Episode 27: Total Reward = -16.999999999999936, Avg Actor Loss = -0.0154, Avg Critic Loss = 0.0390\n",
      "Episode 28: Total Reward = -7.519999999999998, Avg Actor Loss = -0.0705, Avg Critic Loss = 0.1735\n",
      "Episode 29: Total Reward = -23.239999999999934, Avg Actor Loss = -0.0101, Avg Critic Loss = 0.0258\n",
      "Episode 30: Total Reward = -8.389999999999993, Avg Actor Loss = -0.0531, Avg Critic Loss = 0.1314\n",
      "Episode 31: Total Reward = -20.479999999999862, Avg Actor Loss = -0.0119, Avg Critic Loss = 0.0303\n",
      "Episode 32: Total Reward = -24.43999999999998, Avg Actor Loss = -0.0096, Avg Critic Loss = 0.0244\n",
      "Episode 33: Total Reward = -7.849999999999996, Avg Actor Loss = -0.0617, Avg Critic Loss = 0.1489\n",
      "Episode 34: Total Reward = -23.599999999999948, Avg Actor Loss = -0.0100, Avg Critic Loss = 0.0255\n",
      "Episode 35: Total Reward = -25.280000000000012, Avg Actor Loss = -0.0092, Avg Critic Loss = 0.0233\n",
      "Episode 36: Total Reward = -7.639999999999997, Avg Actor Loss = -0.0672, Avg Critic Loss = 0.1627\n",
      "Episode 37: Total Reward = -24.919999999999998, Avg Actor Loss = -0.0093, Avg Critic Loss = 0.0238\n",
      "Episode 38: Total Reward = -21.55999999999987, Avg Actor Loss = -0.0113, Avg Critic Loss = 0.0284\n",
      "Episode 39: Total Reward = -7.759999999999996, Avg Actor Loss = -0.0651, Avg Critic Loss = 0.1592\n",
      "Episode 40: Total Reward = -22.63999999999991, Avg Actor Loss = -0.0105, Avg Critic Loss = 0.0267\n",
      "Episode 41: Total Reward = -18.19999999999991, Avg Actor Loss = -0.0141, Avg Critic Loss = 0.0359\n",
      "Episode 42: Total Reward = -7.8799999999999955, Avg Actor Loss = -0.0621, Avg Critic Loss = 0.1498\n",
      "Episode 43: Total Reward = -22.999999999999925, Avg Actor Loss = -0.0103, Avg Critic Loss = 0.0262\n",
      "Episode 44: Total Reward = -19.999999999999872, Avg Actor Loss = -0.0125, Avg Critic Loss = 0.0315\n",
      "Episode 45: Total Reward = -7.669999999999997, Avg Actor Loss = -0.0662, Avg Critic Loss = 0.1580\n",
      "Episode 46: Total Reward = -22.759999999999916, Avg Actor Loss = -0.0104, Avg Critic Loss = 0.0263\n",
      "Episode 47: Total Reward = -18.439999999999905, Avg Actor Loss = -0.0140, Avg Critic Loss = 0.0349\n",
      "Episode 48: Total Reward = -7.609999999999998, Avg Actor Loss = -0.0684, Avg Critic Loss = 0.1643\n",
      "Episode 49: Total Reward = -22.87999999999992, Avg Actor Loss = -0.0103, Avg Critic Loss = 0.0260\n",
      "Episode 50: Total Reward = -19.519999999999882, Avg Actor Loss = -0.0130, Avg Critic Loss = 0.0325\n",
      "Episode 51: Total Reward = -7.999999999999995, Avg Actor Loss = -0.0592, Avg Critic Loss = 0.1423\n",
      "Episode 52: Total Reward = -17.839999999999918, Avg Actor Loss = -0.0145, Avg Critic Loss = 0.0364\n",
      "Episode 53: Total Reward = -22.87999999999992, Avg Actor Loss = -0.0107, Avg Critic Loss = 0.0270\n",
      "Episode 54: Total Reward = -7.759999999999996, Avg Actor Loss = -0.0646, Avg Critic Loss = 0.1522\n",
      "Episode 55: Total Reward = -22.03999999999989, Avg Actor Loss = -0.0109, Avg Critic Loss = 0.0273\n",
      "Episode 56: Total Reward = -20.839999999999854, Avg Actor Loss = -0.0119, Avg Critic Loss = 0.0295\n",
      "Episode 57: Total Reward = -23.479999999999944, Avg Actor Loss = -0.0102, Avg Critic Loss = 0.0253\n",
      "Episode 58: Total Reward = -22.63999999999991, Avg Actor Loss = -0.0107, Avg Critic Loss = 0.0268\n",
      "Episode 59: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0140, Avg Critic Loss = 0.0344\n",
      "Episode 60: Total Reward = -17.599999999999923, Avg Actor Loss = -0.0151, Avg Critic Loss = 0.0380\n",
      "Episode 61: Total Reward = -7.669999999999997, Avg Actor Loss = -0.0679, Avg Critic Loss = 0.1676\n",
      "Episode 62: Total Reward = -19.279999999999887, Avg Actor Loss = -0.0132, Avg Critic Loss = 0.0330\n",
      "Episode 63: Total Reward = -19.399999999999885, Avg Actor Loss = -0.0131, Avg Critic Loss = 0.0326\n",
      "Episode 64: Total Reward = -8.029999999999994, Avg Actor Loss = -0.0596, Avg Critic Loss = 0.1415\n",
      "Episode 65: Total Reward = -20.239999999999867, Avg Actor Loss = -0.0124, Avg Critic Loss = 0.0312\n",
      "Episode 66: Total Reward = -16.639999999999944, Avg Actor Loss = -0.0163, Avg Critic Loss = 0.0411\n",
      "Episode 67: Total Reward = -21.79999999999988, Avg Actor Loss = -0.0111, Avg Critic Loss = 0.0279\n",
      "Episode 68: Total Reward = -23.839999999999957, Avg Actor Loss = -0.0100, Avg Critic Loss = 0.0249\n",
      "Episode 69: Total Reward = -18.559999999999903, Avg Actor Loss = -0.0142, Avg Critic Loss = 0.0353\n",
      "Episode 70: Total Reward = -21.79999999999988, Avg Actor Loss = -0.0112, Avg Critic Loss = 0.0279\n",
      "Episode 71: Total Reward = -8.239999999999993, Avg Actor Loss = -0.0548, Avg Critic Loss = 0.1305\n",
      "Episode 72: Total Reward = -17.599999999999923, Avg Actor Loss = -0.0149, Avg Critic Loss = 0.0368\n",
      "Episode 73: Total Reward = -23.95999999999996, Avg Actor Loss = -0.0099, Avg Critic Loss = 0.0251\n",
      "Episode 74: Total Reward = -7.6999999999999975, Avg Actor Loss = -0.0666, Avg Critic Loss = 0.1555\n",
      "Episode 75: Total Reward = -19.279999999999887, Avg Actor Loss = -0.0131, Avg Critic Loss = 0.0326\n",
      "Episode 76: Total Reward = -19.63999999999988, Avg Actor Loss = -0.0129, Avg Critic Loss = 0.0324\n",
      "Episode 77: Total Reward = -20.59999999999986, Avg Actor Loss = -0.0119, Avg Critic Loss = 0.0297\n",
      "Episode 78: Total Reward = -26.960000000000075, Avg Actor Loss = -0.0086, Avg Critic Loss = 0.0215\n",
      "Episode 79: Total Reward = -8.659999999999991, Avg Actor Loss = -0.0491, Avg Critic Loss = 0.1151\n",
      "Episode 80: Total Reward = -18.319999999999908, Avg Actor Loss = -0.0139, Avg Critic Loss = 0.0343\n",
      "Episode 81: Total Reward = -18.799999999999898, Avg Actor Loss = -0.0136, Avg Critic Loss = 0.0337\n",
      "Episode 82: Total Reward = -20.779999999999855, Avg Actor Loss = -0.0119, Avg Critic Loss = 0.0296\n",
      "Episode 83: Total Reward = -18.19999999999991, Avg Actor Loss = -0.0143, Avg Critic Loss = 0.0358\n",
      "Episode 84: Total Reward = -20.839999999999854, Avg Actor Loss = -0.0118, Avg Critic Loss = 0.0297\n",
      "Episode 85: Total Reward = -18.559999999999903, Avg Actor Loss = -0.0140, Avg Critic Loss = 0.0349\n",
      "Episode 86: Total Reward = -19.039999999999893, Avg Actor Loss = -0.0133, Avg Critic Loss = 0.0330\n",
      "Episode 87: Total Reward = -19.879999999999875, Avg Actor Loss = -0.0126, Avg Critic Loss = 0.0315\n",
      "Episode 88: Total Reward = -22.63999999999991, Avg Actor Loss = -0.0107, Avg Critic Loss = 0.0267\n",
      "Episode 89: Total Reward = -16.87999999999994, Avg Actor Loss = -0.0159, Avg Critic Loss = 0.0394\n",
      "Episode 90: Total Reward = -7.759999999999996, Avg Actor Loss = -0.0657, Avg Critic Loss = 0.1586\n",
      "Episode 91: Total Reward = -16.75999999999994, Avg Actor Loss = -0.0158, Avg Critic Loss = 0.0388\n",
      "Episode 92: Total Reward = -19.15999999999989, Avg Actor Loss = -0.0132, Avg Critic Loss = 0.0329\n",
      "Episode 93: Total Reward = -18.19999999999991, Avg Actor Loss = -0.0143, Avg Critic Loss = 0.0356\n",
      "Episode 94: Total Reward = -20.479999999999862, Avg Actor Loss = -0.0122, Avg Critic Loss = 0.0308\n",
      "Episode 95: Total Reward = -7.669999999999997, Avg Actor Loss = -0.0680, Avg Critic Loss = 0.1605\n",
      "Episode 96: Total Reward = -18.799999999999898, Avg Actor Loss = -0.0136, Avg Critic Loss = 0.0342\n",
      "Episode 97: Total Reward = -25.52000000000002, Avg Actor Loss = -0.0091, Avg Critic Loss = 0.0228\n",
      "Episode 98: Total Reward = -25.76000000000003, Avg Actor Loss = -0.0091, Avg Critic Loss = 0.0226\n",
      "Episode 99: Total Reward = -17.479999999999926, Avg Actor Loss = -0.0151, Avg Critic Loss = 0.0373\n",
      "Episode 100: Total Reward = -7.849999999999996, Avg Actor Loss = -0.0625, Avg Critic Loss = 0.1478\n",
      "Episode 101: Total Reward = -16.999999999999936, Avg Actor Loss = -0.0156, Avg Critic Loss = 0.0388\n",
      "Episode 102: Total Reward = -20.479999999999862, Avg Actor Loss = -0.0121, Avg Critic Loss = 0.0303\n",
      "Episode 103: Total Reward = -19.519999999999882, Avg Actor Loss = -0.0129, Avg Critic Loss = 0.0323\n",
      "Episode 104: Total Reward = -17.959999999999916, Avg Actor Loss = -0.0146, Avg Critic Loss = 0.0357\n",
      "Episode 105: Total Reward = -8.239999999999993, Avg Actor Loss = -0.0542, Avg Critic Loss = 0.1267\n",
      "Episode 106: Total Reward = -18.439999999999905, Avg Actor Loss = -0.0137, Avg Critic Loss = 0.0337\n",
      "Episode 107: Total Reward = -25.400000000000016, Avg Actor Loss = -0.0090, Avg Critic Loss = 0.0227\n",
      "Episode 108: Total Reward = -20.95999999999985, Avg Actor Loss = -0.0119, Avg Critic Loss = 0.0295\n",
      "Episode 109: Total Reward = -18.919999999999895, Avg Actor Loss = -0.0135, Avg Critic Loss = 0.0332\n",
      "Episode 110: Total Reward = -8.149999999999995, Avg Actor Loss = -0.0562, Avg Critic Loss = 0.1308\n",
      "Episode 111: Total Reward = -18.559999999999903, Avg Actor Loss = -0.0136, Avg Critic Loss = 0.0336\n",
      "Episode 112: Total Reward = -25.880000000000035, Avg Actor Loss = -0.0089, Avg Critic Loss = 0.0226\n",
      "Episode 113: Total Reward = -8.059999999999995, Avg Actor Loss = -0.0602, Avg Critic Loss = 0.1424\n",
      "Episode 114: Total Reward = -18.079999999999913, Avg Actor Loss = -0.0141, Avg Critic Loss = 0.0349\n",
      "Episode 115: Total Reward = -24.43999999999998, Avg Actor Loss = -0.0096, Avg Critic Loss = 0.0239\n",
      "Episode 116: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0139, Avg Critic Loss = 0.0345\n",
      "Episode 117: Total Reward = -23.239999999999934, Avg Actor Loss = -0.0102, Avg Critic Loss = 0.0257\n",
      "Episode 118: Total Reward = -20.839999999999854, Avg Actor Loss = -0.0118, Avg Critic Loss = 0.0292\n",
      "Episode 119: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0137, Avg Critic Loss = 0.0334\n",
      "Episode 120: Total Reward = -7.579999999999998, Avg Actor Loss = -0.0690, Avg Critic Loss = 0.1626\n",
      "Episode 121: Total Reward = -17.71999999999992, Avg Actor Loss = -0.0147, Avg Critic Loss = 0.0367\n",
      "Episode 122: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0136, Avg Critic Loss = 0.0337\n",
      "Episode 123: Total Reward = -24.319999999999975, Avg Actor Loss = -0.0096, Avg Critic Loss = 0.0241\n",
      "Episode 124: Total Reward = -19.249999999999886, Avg Actor Loss = -0.0134, Avg Critic Loss = 0.0326\n",
      "Episode 125: Total Reward = -18.919999999999895, Avg Actor Loss = -0.0131, Avg Critic Loss = 0.0314\n",
      "Episode 126: Total Reward = -23.11999999999993, Avg Actor Loss = -0.0102, Avg Critic Loss = 0.0258\n",
      "Episode 127: Total Reward = -17.71999999999992, Avg Actor Loss = -0.0147, Avg Critic Loss = 0.0360\n",
      "Episode 128: Total Reward = -21.919999999999884, Avg Actor Loss = -0.0110, Avg Critic Loss = 0.0276\n",
      "Episode 129: Total Reward = -19.039999999999893, Avg Actor Loss = -0.0133, Avg Critic Loss = 0.0325\n",
      "Episode 130: Total Reward = -22.63999999999991, Avg Actor Loss = -0.0106, Avg Critic Loss = 0.0267\n",
      "Episode 131: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0137, Avg Critic Loss = 0.0337\n",
      "Episode 132: Total Reward = -22.999999999999925, Avg Actor Loss = -0.0103, Avg Critic Loss = 0.0257\n",
      "Episode 133: Total Reward = -8.029999999999994, Avg Actor Loss = -0.0606, Avg Critic Loss = 0.1454\n",
      "Episode 134: Total Reward = -18.559999999999903, Avg Actor Loss = -0.0137, Avg Critic Loss = 0.0339\n",
      "Episode 135: Total Reward = -23.599999999999948, Avg Actor Loss = -0.0100, Avg Critic Loss = 0.0250\n",
      "Episode 136: Total Reward = -17.599999999999923, Avg Actor Loss = -0.0149, Avg Critic Loss = 0.0361\n",
      "Episode 137: Total Reward = -25.76000000000003, Avg Actor Loss = -0.0090, Avg Critic Loss = 0.0224\n",
      "Episode 138: Total Reward = -7.669999999999997, Avg Actor Loss = -0.0682, Avg Critic Loss = 0.1618\n",
      "Episode 139: Total Reward = -18.19999999999991, Avg Actor Loss = -0.0140, Avg Critic Loss = 0.0348\n",
      "Episode 140: Total Reward = -20.479999999999862, Avg Actor Loss = -0.0119, Avg Critic Loss = 0.0291\n",
      "Episode 141: Total Reward = -16.999999999999936, Avg Actor Loss = -0.0156, Avg Critic Loss = 0.0384\n",
      "Episode 142: Total Reward = -21.55999999999987, Avg Actor Loss = -0.0111, Avg Critic Loss = 0.0276\n",
      "Episode 143: Total Reward = -7.999999999999995, Avg Actor Loss = -0.0597, Avg Critic Loss = 0.1369\n",
      "Episode 144: Total Reward = -18.079999999999913, Avg Actor Loss = -0.0142, Avg Critic Loss = 0.0361\n",
      "Episode 145: Total Reward = -21.439999999999866, Avg Actor Loss = -0.0113, Avg Critic Loss = 0.0283\n",
      "Episode 146: Total Reward = -17.71999999999992, Avg Actor Loss = -0.0147, Avg Critic Loss = 0.0359\n",
      "Episode 147: Total Reward = -18.919999999999895, Avg Actor Loss = -0.0134, Avg Critic Loss = 0.0331\n",
      "Episode 148: Total Reward = -7.759999999999996, Avg Actor Loss = -0.0637, Avg Critic Loss = 0.1454\n",
      "Episode 149: Total Reward = -17.959999999999916, Avg Actor Loss = -0.0143, Avg Critic Loss = 0.0357\n",
      "Episode 150: Total Reward = -18.799999999999898, Avg Actor Loss = -0.0134, Avg Critic Loss = 0.0333\n",
      "Episode 151: Total Reward = -17.599999999999923, Avg Actor Loss = -0.0148, Avg Critic Loss = 0.0361\n",
      "Episode 152: Total Reward = -19.879999999999875, Avg Actor Loss = -0.0125, Avg Critic Loss = 0.0310\n",
      "Episode 153: Total Reward = -7.729999999999997, Avg Actor Loss = -0.0664, Avg Critic Loss = 0.1557\n",
      "Episode 154: Total Reward = -19.519999999999882, Avg Actor Loss = -0.0130, Avg Critic Loss = 0.0339\n",
      "Episode 155: Total Reward = -20.479999999999862, Avg Actor Loss = -0.0122, Avg Critic Loss = 0.0301\n",
      "Episode 156: Total Reward = -18.439999999999905, Avg Actor Loss = -0.0140, Avg Critic Loss = 0.0342\n",
      "Episode 157: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0137, Avg Critic Loss = 0.0336\n",
      "Episode 158: Total Reward = -7.609999999999998, Avg Actor Loss = -0.0668, Avg Critic Loss = 0.1560\n",
      "Episode 159: Total Reward = -21.679999999999875, Avg Actor Loss = -0.0109, Avg Critic Loss = 0.0271\n",
      "Episode 160: Total Reward = -22.03999999999989, Avg Actor Loss = -0.0110, Avg Critic Loss = 0.0271\n",
      "Episode 161: Total Reward = -18.439999999999905, Avg Actor Loss = -0.0139, Avg Critic Loss = 0.0342\n",
      "Episode 162: Total Reward = -24.19999999999997, Avg Actor Loss = -0.0096, Avg Critic Loss = 0.0240\n",
      "Episode 163: Total Reward = -7.729999999999997, Avg Actor Loss = -0.0656, Avg Critic Loss = 0.1499\n",
      "Episode 164: Total Reward = -18.799999999999898, Avg Actor Loss = -0.0133, Avg Critic Loss = 0.0328\n",
      "Episode 165: Total Reward = -24.19999999999997, Avg Actor Loss = -0.0096, Avg Critic Loss = 0.0238\n",
      "Episode 166: Total Reward = -18.319999999999908, Avg Actor Loss = -0.0140, Avg Critic Loss = 0.0336\n",
      "Episode 167: Total Reward = -18.799999999999898, Avg Actor Loss = -0.0133, Avg Critic Loss = 0.0327\n",
      "Episode 168: Total Reward = -18.6799999999999, Avg Actor Loss = -0.0135, Avg Critic Loss = 0.0333\n",
      "Episode 169: Total Reward = -7.549999999999998, Avg Actor Loss = -0.0684, Avg Critic Loss = 0.1530\n",
      "Episode 170: Total Reward = -23.11999999999993, Avg Actor Loss = -0.0099, Avg Critic Loss = 0.0252\n",
      "Episode 171: Total Reward = -20.359999999999864, Avg Actor Loss = -0.0123, Avg Critic Loss = 0.0301\n",
      "Episode 172: Total Reward = -21.199999999999857, Avg Actor Loss = -0.0116, Avg Critic Loss = 0.0288\n",
      "Episode 173: Total Reward = -19.63999999999988, Avg Actor Loss = -0.0129, Avg Critic Loss = 0.0318\n",
      "Episode 174: Total Reward = -7.7899999999999965, Avg Actor Loss = -0.0627, Avg Critic Loss = 0.1433\n",
      "Episode 175: Total Reward = -20.95999999999985, Avg Actor Loss = -0.0115, Avg Critic Loss = 0.0288\n",
      "Episode 176: Total Reward = -18.19999999999991, Avg Actor Loss = -0.0143, Avg Critic Loss = 0.0344\n",
      "Episode 177: Total Reward = -17.71999999999992, Avg Actor Loss = -0.0145, Avg Critic Loss = 0.0358\n"
     ]
    }
   ],
   "source": [
    "gym.register_envs(ale_py)\n",
    "env = gym.make(\"ALE/Tetris-v5\", render_mode=\"human\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "state, _ = env.reset()\n",
    "save_dir = \"training_logs2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(dir(env))\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_dim, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "\n",
    "        # Use a dummy input to calculate the flattened size after conv layers\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, state.shape[0], state.shape[1])\n",
    "            conv_output = self.conv_layers(dummy_input)\n",
    "            conv_output_size = conv_output.view(-1).size(0)\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def conv_layers(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.conv_layers(state)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, state.shape[0], state.shape[1])\n",
    "            conv_output = self.conv_layers(dummy_input)\n",
    "            conv_output_size = conv_output.view(-1).size(0)\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_output_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def conv_layers(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.conv_layers(state)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        state_value = self.fc2(x)\n",
    "        return state_value\n",
    "\n",
    "def calculate_reward(lines_cleared, game_over):\n",
    "    if game_over:\n",
    "        return game_over_penalty\n",
    "    elif lines_cleared > 0:\n",
    "        return line_clear_reward * lines_cleared\n",
    "    else:\n",
    "        return step_penalty\n",
    "\n",
    "def train_step(state, action, reward, next_state, done):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2) / 255.0\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2) / 255.0\n",
    "    action = torch.tensor(action, dtype=torch.long)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "    # Critic update with gradient clipping\n",
    "    value = critic(state)\n",
    "    next_value = critic(next_state) * (1 - done)\n",
    "    target_value = reward + gamma * next_value\n",
    "    critic_loss = (value - target_value.detach()) ** 2\n",
    "\n",
    "    critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)\n",
    "    critic_optimizer.step()\n",
    "\n",
    "    # Actor update with gradient clipping\n",
    "    advantage = (target_value - value).detach()\n",
    "    action_probs = actor(state)\n",
    "    log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "    actor_loss = -log_prob * advantage\n",
    "\n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.0)\n",
    "    actor_optimizer.step()\n",
    "\n",
    "    return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "# Actor and Critic networks and optimizers\n",
    "actor = Actor(action_dim=env.action_space.n)\n",
    "critic = Critic()\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n",
    "print(action_dim)\n",
    "\n",
    "# parameters and metrics\n",
    "def save_parameters_and_metrics(episode, actor, critic, actor_optimizer, critic_optimizer, metrics):\n",
    "    torch.save({\n",
    "        'actor_state_dict': actor.state_dict(),\n",
    "        'critic_state_dict': critic.state_dict(),\n",
    "        'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "        'critic_optimizer_state_dict': critic_optimizer.state_dict()\n",
    "    }, os.path.join(save_dir, f\"model_checkpoint_ep{episode}.pt\"))\n",
    "\n",
    "    with open(os.path.join(save_dir, \"training_metrics.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(metrics, f)\n",
    "\n",
    "metrics = {\n",
    "    \"episode_rewards\": [],\n",
    "    \"step_actor_losses\": [],\n",
    "    \"step_critic_losses\": []\n",
    "}\n",
    "\n",
    "# Define a function to print the average weights and losses\n",
    "def print_parameters_and_losses(actor, critic, episode, avg_actor_loss, avg_critic_loss):\n",
    "    print(f\"\\n--- Statistics at Episode {episode} ---\")\n",
    "    \n",
    "    # Print the average weight of specific layers\n",
    "    actor_conv1_avg = actor.conv1.weight.data.mean().item()\n",
    "    print(f\"Actor Layer 1 Conv Weights Average: {actor_conv1_avg}\")\n",
    "    \n",
    "    actor_fc1_avg = actor.fc1.weight.data.mean().item()\n",
    "    print(f\"Actor Layer 1 FC Weights Average: {actor_fc1_avg}\")\n",
    "    \n",
    "    critic_conv1_avg = critic.conv1.weight.data.mean().item()\n",
    "    print(f\"Critic Layer 1 Conv Weights Average: {critic_conv1_avg}\")\n",
    "    \n",
    "    critic_fc1_avg = critic.fc1.weight.data.mean().item()\n",
    "    print(f\"Critic Layer 1 FC Weights Average: {critic_fc1_avg}\")\n",
    "\n",
    "    # Print average losses for actor and critic\n",
    "    print(f\"Average Actor Loss: {avg_actor_loss}\")\n",
    "    print(f\"Average Critic Loss: {avg_critic_loss}\")\n",
    "    \n",
    "    print(\"--- End of Statistics ---\\n\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "\n",
    "    while not done:\n",
    "        # Preprocess state for model input\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2) / 255.0\n",
    "\n",
    "        # Get action probabilities from the actor network\n",
    "        action_probs = actor(state_tensor).detach().numpy().squeeze()\n",
    "\n",
    "        # ε-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Exploration: choose a random action\n",
    "            action = np.random.choice(action_dim)\n",
    "        else:\n",
    "            # Exploitation: choose the action with the highest probability\n",
    "            action = np.argmax(action_probs)\n",
    "\n",
    "        # Decay epsilon after each step\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, _, done, _, info = env.step(action)\n",
    "        \n",
    "        # Calculate reward\n",
    "        lines_cleared = info.get(\"lines_cleared\", 0)\n",
    "        reward = calculate_reward(lines_cleared, done)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Train actor and critic networks\n",
    "        actor_loss, critic_loss = train_step(state, action, reward, next_state, done)\n",
    "\n",
    "        # Log losses for each step\n",
    "        actor_losses.append(actor_loss)\n",
    "        critic_losses.append(critic_loss)\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "    # Calculate average losses for this episode\n",
    "    avg_actor_loss = np.mean(actor_losses)\n",
    "    avg_critic_loss = np.mean(critic_losses)\n",
    "\n",
    "    # Print the episode results with average losses\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {episode_reward}, Avg Actor Loss = {avg_actor_loss:.4f}, Avg Critic Loss = {avg_critic_loss:.4f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6e67c4e-e10c-45ff-9942-5dda2cf04361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation structure: [[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [ 50 132  50]\n",
      "  [ 50 132  50]\n",
      "  [ 50 132  50]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]]\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment and get the initial observation\n",
    "observation, _ = env.reset()\n",
    "print(\"Observation structure:\", observation)  # Add this line to inspect the observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b2a67-5954-4550-ba15-138623c30538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
